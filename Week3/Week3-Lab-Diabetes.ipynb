{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ef7207",
   "metadata": {},
   "source": [
    "# Exploring Lasso and Ridge Regression, kNN, and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99c5f1",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Welcome to this hands-on lab where we will dive deep into some essential machine learning techniques. By the end of this lab, executed within this Jupyter Notebook, you should have a more tangible grasp of Lasso and Ridge Regression, k-Nearest Neighbors (kNN), and Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695d04",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "\n",
    "- Understand the importance of regularization in preventing overfitting.\n",
    "- Differentiate between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "- Apply Lasso and Ridge regression to a dataset and observe the effect on model coefficients.\n",
    "- Implement k-fold corss validation to fine tune the impact of regularisation (value of alpha)\n",
    "- Grasp the underlying concept behind instance-based learning and how kNN makes predictions based on data proximity.\n",
    "- Explore the effect of the hyperparameter 'k' on model performance.\n",
    "- Delve into the significance of distance metrics in kNN.\n",
    "\n",
    "\n",
    "As we navigate through the lab, there will be hands-on exercises, reflection points, and visualization segments to reinforce the concepts and allow you to observe the real-world implications of these techniques.\n",
    "\n",
    "Let's embark on this exciting journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a898079",
   "metadata": {},
   "source": [
    "\n",
    "# # 2. Diabetes Dataset from Scikit-learn\n",
    "\n",
    "The Diabetes dataset from Scikit-learn is a well-known dataset used in regression problems. It contains medical data from diabetic patients and is often used to predict disease progression based on various health indicators.\n",
    "\n",
    "- **Number of Instances (samples):** 442\n",
    "- **Number of Attributes (features):** 10\n",
    "- **Type of Problem:** Regression (predicts a continuous value, which is a quantitative measure of disease progression one year after baseline)\n",
    "\n",
    "### Features\n",
    "The dataset includes the following 10 baseline variables (features), all of which are numerical and standardized:\n",
    "\n",
    "1. **age**: Age of the patient.\n",
    "2. **sex**: Gender of the patient.\n",
    "3. **bmi**: Body Mass Index.\n",
    "4. **bp**: Average blood pressure.\n",
    "5. **s1**: T-Cells (a type of white blood cells).\n",
    "6. **s2**: Low-Density Lipoproteins (LDL cholesterol).\n",
    "7. **s3**: High-Density Lipoproteins (HDL cholesterol).\n",
    "8. **s4**: Thyroid Stimulating Hormone (TSH).\n",
    "9. **s5**: Lamotrigine concentration.\n",
    "10. **s6**: Blood sugar level.\n",
    "\n",
    "### Target Variable\n",
    "- A quantitative measure of disease progression one year after the baseline.\n",
    "\n",
    "The data is already pre-processed and normalized, which means each feature's mean is 0 and its standard deviation is 1. This normalization makes the features comparable and helps with the training of regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95913b7",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5552052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Disease_Progression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  Disease_Progression  \n",
       "0 -0.002592  0.019908 -0.017646                151.0  \n",
       "1 -0.039493 -0.068330 -0.092204                 75.0  \n",
       "2 -0.002592  0.002864 -0.025930                141.0  \n",
       "3  0.034309  0.022692 -0.009362                206.0  \n",
       "4 -0.002592 -0.031991 -0.046641                135.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_data = load_diabetes()\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "diabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "\n",
    "# Add the target variable (disease progression) to the DataFrame\n",
    "diabetes_df['Disease_Progression'] = diabetes_data.target\n",
    "\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fd99d",
   "metadata": {},
   "source": [
    "## 3. Lasso and Ridge Regression\n",
    "\n",
    "Through this section, we aim to:\n",
    "\n",
    "- Fit a basic linear regression model and observe its potential for overfitting with many features.\n",
    "- Apply Lasso (L1) and Ridge (L2) regularization and observe their effects on the model's coefficients and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9f02",
   "metadata": {},
   "source": [
    "### Task: Simple Linear Regression without Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604a835",
   "metadata": {},
   "source": [
    "1. **Split the Data**:\n",
    "   - Use `train_test_split` from `sklearn.model_selection` to divide your data into training and testing sets (70% training, 30% testing).\n",
    "\n",
    "2. **Train the Model**:\n",
    "   - Train a **Linear Regression** model using the training data.\n",
    "   - Display the coefficients of the trained model to understand the contribution of each feature.\n",
    "\n",
    "3. **Evaluate the Model**:\n",
    "   - Compute and print the model's score (RÂ²) on both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ca87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  feature  coefficient\n",
      "0     age    29.250346\n",
      "1     sex  -261.707681\n",
      "2     bmi   546.297373\n",
      "3      bp   388.400773\n",
      "4      s1  -901.953387\n",
      "5      s2   506.761149\n",
      "6      s3   121.148459\n",
      "7      s4   288.029325\n",
      "8      s5   659.271338\n",
      "9      s6    41.375369\n",
      "Train score: 0.5244\n",
      "Test score: 0.4773\n"
     ]
    }
   ],
   "source": [
    "# Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aef7b5",
   "metadata": {},
   "source": [
    " Some coefficients have become particularly high, which can be a sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4533e1",
   "metadata": {},
   "source": [
    "### Lasso (L1) and Ridge (L2) Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b38b4",
   "metadata": {},
   "source": [
    "Below, I have implemented Lasso Regression for you. We'll train the model, print the feature coefficients, and evaluate its performance on both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3276909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients:\n",
      "  feature  coefficient\n",
      "0     age     0.000000\n",
      "1     sex    -0.000000\n",
      "2     bmi   443.702165\n",
      "3      bp    51.603401\n",
      "4      s1     0.000000\n",
      "5      s2     0.000000\n",
      "6      s3    -0.000000\n",
      "7      s4     0.000000\n",
      "8      s5   201.967127\n",
      "9      s6     0.000000\n",
      "\n",
      "Lasso Train score: 0.3562\n",
      "Lasso Test score: 0.3619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_coeffs = pd.DataFrame({'feature': X.columns, 'coefficient': lasso.coef_})\n",
    "print(\"Lasso Coefficients:\")\n",
    "print(lasso_coeffs)\n",
    "\n",
    "# Scoring\n",
    "lasso_train_score = lasso.score(X_train, y_train)\n",
    "lasso_test_score = lasso.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nLasso Train score: {lasso_train_score:.4f}\")\n",
    "print(f\"Lasso Test score: {lasso_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbca3bc",
   "metadata": {},
   "source": [
    "Now it's your turn! Follow the same steps above, but implement **Ridge Regression**. Here's what you need to do:\n",
    "\n",
    "1. Import the `Ridge` model from `sklearn.linear_model`.\n",
    "2. Initialize the `Ridge` model with `alpha=1.0`.\n",
    "3. Fit the Ridge model using the training data (`X_train`, `y_train`).\n",
    "4. Display the coefficients of the Ridge model.\n",
    "5. Compute and print the Ridge model's scores for both training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07276ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Coefficients:\n",
      "  feature  coefficient\n",
      "0     age    45.053767\n",
      "1     sex   -71.947551\n",
      "2     bmi   280.715875\n",
      "3      bp   195.213699\n",
      "4      s1    -2.229433\n",
      "5      s2   -17.541159\n",
      "6      s3  -148.688994\n",
      "7      s4   120.467093\n",
      "8      s5   198.614859\n",
      "9      s6   106.934534\n",
      "Ridge Train score: 0.4283\n",
      "Ridge Test score: 0.4233\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fed883",
   "metadata": {},
   "source": [
    "By comparing the coefficients and scores of the basic linear regression model with those of the Lasso and Ridge models, we can observe:\n",
    "\n",
    "- Lasso regression might set some coefficients to zero, effectively selecting a subset of the features.\n",
    "- Ridge regression tends to shrink the coefficients but generally doesn't set them to zero.\n",
    "- Regularization may lead to a model that generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878adced",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning alpha with k-Fold Cross-validation for L1 and L2 Regularization\n",
    "\n",
    "Cross-validation (CV) is a robust method for assessing the performance of machine learning models and for hyperparameter tuning. By dividing our data into 'k' folds and training and testing our model k times, we can get a more accurate measure of its performance.\n",
    "\n",
    "In the context of Lasso (L1) and Ridge (L2) regression, the hyperparameter we're most interested in tuning is \n",
    "Î± (alpha), which determines the strength of the regularization. A higher value of Î± increases the regularization strength, penalizing high coefficients more severely.\n",
    "\n",
    "Let's employ GridSearchCV from sklearn, which performs cross-validation and grid search for hyperparameter tuning simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed52da",
   "metadata": {},
   "source": [
    "**Below**, I have implemented GridSearchCV to find the best alpha for Lasso Regression. The model is trained using various values of alpha and evaluated using a 5-fold cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0de4d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression: Best Î± = 0.01 with R^2 score = 0.45258380038989293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define a list of alphas to check\n",
    "alphas = np.logspace(-6, 6, 13)\n",
    "\n",
    "# Lasso Regression with CV\n",
    "lasso = Lasso()\n",
    "parameters = {'alpha': alphas}\n",
    "lasso_regressor = GridSearchCV(lasso, parameters, scoring='r2', cv=5)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression: Best Î± =\", lasso_regressor.best_params_['alpha'], \"with R^2 score =\", lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d097",
   "metadata": {},
   "source": [
    "### Your Task: Ridge Regression with GridSearchCV\n",
    "\n",
    "Now, it's your turn to apply the same procedure for **Ridge Regression**. Here's what you need to do:\n",
    "\n",
    "1. Import the `Ridge` model from `sklearn.linear_model`.\n",
    "2. Initialize the `Ridge` model.\n",
    "3. Define a grid of `alpha` values using `np.logspace(-6, 6, 13)`.\n",
    "4. Use `GridSearchCV` to train and evaluate the Ridge model using 5-fold cross-validation.\n",
    "5. Print the best `alpha` value and the corresponding RÂ² score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab74241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: Best Î± = 0.001 with R^2 score = 0.45289008081681914\n"
     ]
    }
   ],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adba9c1",
   "metadata": {},
   "source": [
    "With the best Î± values obtained from the k-fold cross-validation, we can retrain our Lasso and Ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca55990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain using best alphas\n",
    "optimal_alpha_lasso = lasso_regressor.best_params_['alpha']\n",
    "optimal_alpha_ridge = ridge_regressor.best_params_['alpha']\n",
    "\n",
    "lasso_optimal = Lasso(alpha=optimal_alpha_lasso)\n",
    "ridge_optimal = Ridge(alpha=optimal_alpha_ridge)\n",
    "\n",
    "lasso_optimal.fit(X_train, y_train)\n",
    "ridge_optimal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562c8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Lasso: Train score = 0.5231, Test score = 0.4787\n",
      "Optimal Ridge: Train score = 0.5243, Test score = 0.4775\n"
     ]
    }
   ],
   "source": [
    "lasso_optimal_train_score = lasso_optimal.score(X_train, y_train)\n",
    "lasso_optimal_test_score = lasso_optimal.score(X_test, y_test)\n",
    "ridge_optimal_train_score = ridge_optimal.score(X_train, y_train)\n",
    "ridge_optimal_test_score = ridge_optimal.score(X_test, y_test)\n",
    "\n",
    "# Print out scores for comparison\n",
    "print(\"Optimal Lasso: Train score = {:.4f}, Test score = {:.4f}\".format(lasso_optimal_train_score, lasso_optimal_test_score))\n",
    "print(\"Optimal Ridge: Train score = {:.4f}, Test score = {:.4f}\".format(ridge_optimal_train_score, ridge_optimal_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e0a55",
   "metadata": {},
   "source": [
    "From the results, we can observe whether the retrained models (with optimal \n",
    "Î±) show better or similar performance to the original ones. Usually, the optimal models should generalize better, which would be evident from a better or comparable test score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a24f56",
   "metadata": {},
   "source": [
    "## 5. k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a6064",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (kNN) algorithm is a simple, intuitive, non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. For classification, the output is a class membership: an object is classified by a majority vote of its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92f8fc",
   "metadata": {},
   "source": [
    "### Implementing kNN Regression\n",
    "\n",
    "First, let's use kNN as a regression tool on the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbb5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Regressor (k=5): Train score = 0.5727, Test score = 0.4031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize kNN regressor with k=5\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and score\n",
    "knn_train_score = knn_regressor.score(X_train, y_train)\n",
    "knn_test_score = knn_regressor.score(X_test, y_test)\n",
    "\n",
    "print(f\"kNN Regressor (k=5): Train score = {knn_train_score:.4f}, Test score = {knn_test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004482d",
   "metadata": {},
   "source": [
    "## Student Task: Fine-tuning k\n",
    "\n",
    "Choosing the right k value is crucial. A small k can be noisy and subject to outliers, while a large k can be computationally intensive and might smoothen the output excessively. Accordingly,\n",
    "\n",
    "- Fine tune k using Cross-validation\n",
    "- Retrain kNN with optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
