{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ef7207",
   "metadata": {},
   "source": [
    "# Exploring Lasso and Ridge Regression, kNN, and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99c5f1",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Welcome to this hands-on lab where we will dive deep into some essential machine learning techniques. By the end of this lab, executed within this Jupyter Notebook, you should have a more tangible grasp of Lasso and Ridge Regression, k-Nearest Neighbors (kNN), and Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d695d04",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "\n",
    "- Understand the importance of regularization in preventing overfitting.\n",
    "- Differentiate between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "- Apply Lasso and Ridge regression to a dataset and observe the effect on model coefficients.\n",
    "- Implement k-fold corss validation to fine tune the impact of regularisation (value of alpha)\n",
    "- Grasp the underlying concept behind instance-based learning and how kNN makes predictions based on data proximity.\n",
    "- Explore the effect of the hyperparameter 'k' on model performance.\n",
    "- Delve into the significance of distance metrics in kNN.\n",
    "\n",
    "\n",
    "As we navigate through the lab, there will be hands-on exercises, reflection points, and visualization segments to reinforce the concepts and allow you to observe the real-world implications of these techniques.\n",
    "\n",
    "Let's embark on this exciting journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a898079",
   "metadata": {},
   "source": [
    "\n",
    "# # 2. Diabetes Dataset from Scikit-learn\n",
    "\n",
    "The Diabetes dataset from Scikit-learn is a well-known dataset used in regression problems. It contains medical data from diabetic patients and is often used to predict disease progression based on various health indicators.\n",
    "\n",
    "- **Number of Instances (samples):** 442\n",
    "- **Number of Attributes (features):** 10\n",
    "- **Type of Problem:** Regression (predicts a continuous value, which is a quantitative measure of disease progression one year after baseline)\n",
    "\n",
    "### Features\n",
    "The dataset includes the following 10 baseline variables (features), all of which are numerical and standardized:\n",
    "\n",
    "1. **age**: Age of the patient.\n",
    "2. **sex**: Gender of the patient.\n",
    "3. **bmi**: Body Mass Index.\n",
    "4. **bp**: Average blood pressure.\n",
    "5. **s1**: T-Cells (a type of white blood cells).\n",
    "6. **s2**: Low-Density Lipoproteins (LDL cholesterol).\n",
    "7. **s3**: High-Density Lipoproteins (HDL cholesterol).\n",
    "8. **s4**: Thyroid Stimulating Hormone (TSH).\n",
    "9. **s5**: Lamotrigine concentration.\n",
    "10. **s6**: Blood sugar level.\n",
    "\n",
    "### Target Variable\n",
    "- A quantitative measure of disease progression one year after the baseline.\n",
    "\n",
    "The data is already pre-processed and normalized, which means each feature's mean is 0 and its standard deviation is 1. This normalization makes the features comparable and helps with the training of regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95913b7",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5552052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Disease_Progression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  Disease_Progression  \n",
       "0 -0.002592  0.019908 -0.017646                151.0  \n",
       "1 -0.039493 -0.068330 -0.092204                 75.0  \n",
       "2 -0.002592  0.002864 -0.025930                141.0  \n",
       "3  0.034309  0.022692 -0.009362                206.0  \n",
       "4 -0.002592 -0.031991 -0.046641                135.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_data = load_diabetes()\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "diabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "\n",
    "# Add the target variable (disease progression) to the DataFrame\n",
    "diabetes_df['Disease_Progression'] = diabetes_data.target\n",
    "\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fd99d",
   "metadata": {},
   "source": [
    "## 3. Lasso and Ridge Regression\n",
    "\n",
    "Through this section, we aim to:\n",
    "\n",
    "- Fit a basic linear regression model and observe its potential for overfitting with many features.\n",
    "- Apply Lasso (L1) and Ridge (L2) regularization and observe their effects on the model's coefficients and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9f02",
   "metadata": {},
   "source": [
    "### Task: Simple Linear Regression without Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604a835",
   "metadata": {},
   "source": [
    "1. **Split the Data**:\n",
    "   - Use `train_test_split` from `sklearn.model_selection` to divide your data into training and testing sets (70% training, 30% testing).\n",
    "\n",
    "2. **Train the Model**:\n",
    "   - Train a **Linear Regression** model using the training data.\n",
    "   - Display the coefficients of the trained model to understand the contribution of each feature.\n",
    "\n",
    "3. **Evaluate the Model**:\n",
    "   - Compute and print the model's score (R²) on both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ca87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  feature  coefficient\n",
      "0     age    29.250346\n",
      "1     sex  -261.707681\n",
      "2     bmi   546.297373\n",
      "3      bp   388.400773\n",
      "4      s1  -901.953387\n",
      "5      s2   506.761149\n",
      "6      s3   121.148459\n",
      "7      s4   288.029325\n",
      "8      s5   659.271338\n",
      "9      s6    41.375369\n",
      "Train score: 0.5244\n",
      "Test score: 0.4773\n"
     ]
    }
   ],
   "source": [
    "# Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aef7b5",
   "metadata": {},
   "source": [
    " Some coefficients have become particularly high, which can be a sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4533e1",
   "metadata": {},
   "source": [
    "### Lasso (L1) and Ridge (L2) Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b38b4",
   "metadata": {},
   "source": [
    "Below, I have implemented Lasso Regression for you. We'll train the model, print the feature coefficients, and evaluate its performance on both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3276909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients:\n",
      "  feature  coefficient\n",
      "0     age     0.000000\n",
      "1     sex    -0.000000\n",
      "2     bmi   443.702165\n",
      "3      bp    51.603401\n",
      "4      s1     0.000000\n",
      "5      s2     0.000000\n",
      "6      s3    -0.000000\n",
      "7      s4     0.000000\n",
      "8      s5   201.967127\n",
      "9      s6     0.000000\n",
      "\n",
      "Lasso Train score: 0.3562\n",
      "Lasso Test score: 0.3619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_coeffs = pd.DataFrame({'feature': X.columns, 'coefficient': lasso.coef_})\n",
    "print(\"Lasso Coefficients:\")\n",
    "print(lasso_coeffs)\n",
    "\n",
    "# Scoring\n",
    "lasso_train_score = lasso.score(X_train, y_train)\n",
    "lasso_test_score = lasso.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nLasso Train score: {lasso_train_score:.4f}\")\n",
    "print(f\"Lasso Test score: {lasso_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbca3bc",
   "metadata": {},
   "source": [
    "Now it's your turn! Follow the same steps above, but implement **Ridge Regression**. Here's what you need to do:\n",
    "\n",
    "1. Import the `Ridge` model from `sklearn.linear_model`.\n",
    "2. Initialize the `Ridge` model with `alpha=1.0`.\n",
    "3. Fit the Ridge model using the training data (`X_train`, `y_train`).\n",
    "4. Display the coefficients of the Ridge model.\n",
    "5. Compute and print the Ridge model's scores for both training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07276ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Coefficients:\n",
      "  feature  coefficient\n",
      "0     age    45.053767\n",
      "1     sex   -71.947551\n",
      "2     bmi   280.715875\n",
      "3      bp   195.213699\n",
      "4      s1    -2.229433\n",
      "5      s2   -17.541159\n",
      "6      s3  -148.688994\n",
      "7      s4   120.467093\n",
      "8      s5   198.614859\n",
      "9      s6   106.934534\n",
      "Ridge Train score: 0.4283\n",
      "Ridge Test score: 0.4233\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fed883",
   "metadata": {},
   "source": [
    "By comparing the coefficients and scores of the basic linear regression model with those of the Lasso and Ridge models, we can observe:\n",
    "\n",
    "- Lasso regression might set some coefficients to zero, effectively selecting a subset of the features.\n",
    "- Ridge regression tends to shrink the coefficients but generally doesn't set them to zero.\n",
    "- Regularization may lead to a model that generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878adced",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning alpha with k-Fold Cross-validation for L1 and L2 Regularization\n",
    "\n",
    "Cross-validation (CV) is a robust method for assessing the performance of machine learning models and for hyperparameter tuning. By dividing our data into 'k' folds and training and testing our model k times, we can get a more accurate measure of its performance.\n",
    "\n",
    "In the context of Lasso (L1) and Ridge (L2) regression, the hyperparameter we're most interested in tuning is \n",
    "α (alpha), which determines the strength of the regularization. A higher value of α increases the regularization strength, penalizing high coefficients more severely.\n",
    "\n",
    "Let's employ GridSearchCV from sklearn, which performs cross-validation and grid search for hyperparameter tuning simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed52da",
   "metadata": {},
   "source": [
    "**Below**, I have implemented GridSearchCV to find the best alpha for Lasso Regression. The model is trained using various values of alpha and evaluated using a 5-fold cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0de4d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression: Best α = 0.01 with R^2 score = 0.45258380038989293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define a list of alphas to check\n",
    "alphas = np.logspace(-6, 6, 13)\n",
    "\n",
    "# Lasso Regression with CV\n",
    "lasso = Lasso()\n",
    "parameters = {'alpha': alphas}\n",
    "lasso_regressor = GridSearchCV(lasso, parameters, scoring='r2', cv=5)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso Regression: Best α =\", lasso_regressor.best_params_['alpha'], \"with R^2 score =\", lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d097",
   "metadata": {},
   "source": [
    "### Your Task: Ridge Regression with GridSearchCV\n",
    "\n",
    "Now, it's your turn to apply the same procedure for **Ridge Regression**. Here's what you need to do:\n",
    "\n",
    "1. Import the `Ridge` model from `sklearn.linear_model`.\n",
    "2. Initialize the `Ridge` model.\n",
    "3. Define a grid of `alpha` values using `np.logspace(-6, 6, 13)`.\n",
    "4. Use `GridSearchCV` to train and evaluate the Ridge model using 5-fold cross-validation.\n",
    "5. Print the best `alpha` value and the corresponding R² score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab74241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: Best α = 0.001 with R^2 score = 0.45289008081681914\n"
     ]
    }
   ],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adba9c1",
   "metadata": {},
   "source": [
    "With the best α values obtained from the k-fold cross-validation, we can retrain our Lasso and Ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca55990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain using best alphas\n",
    "optimal_alpha_lasso = lasso_regressor.best_params_['alpha']\n",
    "optimal_alpha_ridge = ridge_regressor.best_params_['alpha']\n",
    "\n",
    "lasso_optimal = Lasso(alpha=optimal_alpha_lasso)\n",
    "ridge_optimal = Ridge(alpha=optimal_alpha_ridge)\n",
    "\n",
    "lasso_optimal.fit(X_train, y_train)\n",
    "ridge_optimal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562c8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Lasso: Train score = 0.5231, Test score = 0.4787\n",
      "Optimal Ridge: Train score = 0.5243, Test score = 0.4775\n"
     ]
    }
   ],
   "source": [
    "lasso_optimal_train_score = lasso_optimal.score(X_train, y_train)\n",
    "lasso_optimal_test_score = lasso_optimal.score(X_test, y_test)\n",
    "ridge_optimal_train_score = ridge_optimal.score(X_train, y_train)\n",
    "ridge_optimal_test_score = ridge_optimal.score(X_test, y_test)\n",
    "\n",
    "# Print out scores for comparison\n",
    "print(\"Optimal Lasso: Train score = {:.4f}, Test score = {:.4f}\".format(lasso_optimal_train_score, lasso_optimal_test_score))\n",
    "print(\"Optimal Ridge: Train score = {:.4f}, Test score = {:.4f}\".format(ridge_optimal_train_score, ridge_optimal_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e0a55",
   "metadata": {},
   "source": [
    "From the results, we can observe whether the retrained models (with optimal \n",
    "α) show better or similar performance to the original ones. Usually, the optimal models should generalize better, which would be evident from a better or comparable test score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a24f56",
   "metadata": {},
   "source": [
    "## 5. k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a6064",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (kNN) algorithm is a simple, intuitive, non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. For classification, the output is a class membership: an object is classified by a majority vote of its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92f8fc",
   "metadata": {},
   "source": [
    "### Implementing kNN Regression\n",
    "\n",
    "First, let's use kNN as a regression tool on the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbb5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Regressor (k=5): Train score = 0.5727, Test score = 0.4031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize kNN regressor with k=5\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and score\n",
    "knn_train_score = knn_regressor.score(X_train, y_train)\n",
    "knn_test_score = knn_regressor.score(X_test, y_test)\n",
    "\n",
    "print(f\"kNN Regressor (k=5): Train score = {knn_train_score:.4f}, Test score = {knn_test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004482d",
   "metadata": {},
   "source": [
    "## Student Task: Fine-tuning k\n",
    "\n",
    "Choosing the right k value is crucial. A small k can be noisy and subject to outliers, while a large k can be computationally intensive and might smoothen the output excessively. Accordingly,\n",
    "\n",
    "- Fine tune k using Cross-validation\n",
    "- Retrain kNN with optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
